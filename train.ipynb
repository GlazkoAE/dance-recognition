{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchmetrics\n",
    "import seaborn as sns\n",
    "import torch.utils.data\n",
    "import pytorchvideo.data\n",
    "import pytorch_lightning\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    CenterCrop,\n",
    "    RandomHorizontalFlip,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def move_videos_to_dir(labels: pd.DataFrame, out_dir: Path):\n",
    "    in_dir = Path(\"data\", \"videos\")\n",
    "    for _, label in tqdm(labels.iterrows(), total=len(labels)):\n",
    "        class_name = label[\"label\"]\n",
    "        class_dir = out_dir / class_name\n",
    "        filename = label[\"youtube_id\"] + \".mp4\"\n",
    "        file = in_dir / filename\n",
    "        if file.is_file():\n",
    "            if not class_dir.is_dir():\n",
    "                Path.mkdir(class_dir)\n",
    "            file.rename(class_dir / filename)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "csv_filenames = {\n",
    "    \"train\": \"dancing-train.csv\",\n",
    "    \"val\": \"dancing-validate.csv\",\n",
    "}\n",
    "for phase, filename in csv_filenames.items():\n",
    "    labels_df = pd.read_csv(f\"data/{filename}\")\n",
    "    move_videos_to_dir(labels_df, Path(\"data\", \"videos\", phase))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Init data module"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "class KineticsDataModule(pytorch_lightning.LightningDataModule):\n",
    "\n",
    "  # Dataset configuration\n",
    "  _DATA_PATH = \"./data/videos\"\n",
    "  _CLIP_DURATION = 2  # Duration of sampled clip for each video\n",
    "  _BATCH_SIZE = 4\n",
    "  _NUM_WORKERS = 0  # Number of parallel processes fetching data\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    \"\"\"\n",
    "    Create the Kinetics train partition from the list of video labels\n",
    "    in {self._DATA_PATH}/train\n",
    "    \"\"\"\n",
    "    train_transform = Compose([\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose([\n",
    "                UniformTemporalSubsample(16),\n",
    "                Lambda(lambda x: x / 255.0),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                RandomShortSideScale(min_size=256, max_size=320),\n",
    "                RandomCrop(224),\n",
    "                RandomHorizontalFlip(p=0.5),\n",
    "            ]),\n",
    "        ),\n",
    "    ])\n",
    "    train_dataset = pytorchvideo.data.Kinetics(\n",
    "        data_path=os.path.join(self._DATA_PATH, \"train\"),\n",
    "        clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", self._CLIP_DURATION),\n",
    "        transform=train_transform,\n",
    "        decode_audio=False,\n",
    "    )\n",
    "    return torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=self._BATCH_SIZE,\n",
    "        num_workers=self._NUM_WORKERS,\n",
    "    )\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    \"\"\"\n",
    "    Create the Kinetics validation partition from the list of video labels\n",
    "    in {self._DATA_PATH}/val\n",
    "    \"\"\"\n",
    "    val_transform = Compose([\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose([\n",
    "                UniformTemporalSubsample(16),\n",
    "                Lambda(lambda x: x / 255.0),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                ShortSideScale(224),\n",
    "                CenterCrop(224),\n",
    "                RandomHorizontalFlip(p=0.5),\n",
    "            ]),\n",
    "        ),\n",
    "    ])\n",
    "    val_dataset = pytorchvideo.data.Kinetics(\n",
    "        data_path=os.path.join(self._DATA_PATH, \"val\"),\n",
    "        clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", self._CLIP_DURATION),\n",
    "        transform=val_transform,\n",
    "        decode_audio=False,\n",
    "    )\n",
    "    return torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=self._BATCH_SIZE,\n",
    "        num_workers=self._NUM_WORKERS,\n",
    "    )\n",
    "\n",
    "  def get_classes(self):\n",
    "      folder = os.path.join(self._DATA_PATH, \"train\")\n",
    "      return [f for f in os.listdir(folder)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Init torch lightning trainer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class VideoClassificationLightningModule(pytorch_lightning.LightningModule):\n",
    "    def __init__(self, classes, model_fn):\n",
    "        super().__init__()\n",
    "        self.model = model_fn()\n",
    "        self.accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=15)\n",
    "        self.f1 = torchmetrics.F1Score(task=\"multiclass\", average=\"weighted\", num_classes=15)\n",
    "        self.classes = classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y_hat = self.model(batch[\"video\"])\n",
    "        loss = F.cross_entropy(y_hat, batch[\"label\"])\n",
    "        self.log(\"train_loss\", loss.item(), batch_size=len(batch))\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        preds = self.model(batch[\"video\"])\n",
    "        loss = F.cross_entropy(preds, batch[\"label\"])\n",
    "        self.accuracy(preds, batch[\"label\"])\n",
    "        self.f1(preds, batch[\"label\"])\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return {\"loss\": loss, \"preds\": preds, \"target\": batch[\"label\"]}\n",
    "\n",
    "    def validation_epoch_end(self, outs):\n",
    "        preds = torch.cat([tmp['preds'] for tmp in outs]).to('cpu').numpy()\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        preds = [self.classes[i] for i in preds]\n",
    "        targets = torch.cat([tmp['target'] for tmp in outs]).to('cpu').numpy()\n",
    "        targets = [self.classes[i] for i in targets]\n",
    "        cf_matrix = confusion_matrix(targets, preds, labels=self.classes)\n",
    "\n",
    "        plt.figure(figsize = (10, 7))\n",
    "        fig_ = sns.heatmap(\n",
    "            cf_matrix/np.sum(cf_matrix),\n",
    "            annot=True,\n",
    "            fmt='.1%',\n",
    "            cmap='Blues'\n",
    "        ).get_figure()\n",
    "        plt.close(fig_)\n",
    "\n",
    "        self.logger.experiment.add_figure(\n",
    "            \"Val confusion matrix\",\n",
    "            fig_,\n",
    "            self.current_epoch,\n",
    "        )\n",
    "\n",
    "        self.log(\"val_acc_epoch\", self.accuracy)\n",
    "        self.log(\"val_f1_epoch\", self.f1)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n",
    "        outputs = {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": scheduler,\n",
    "        }\n",
    "        return outputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Init models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def make_kinetics_slow():\n",
    "    model = torch.hub.load('facebookresearch/pytorchvideo', 'slow_r50', pretrained=False)\n",
    "    model.blocks[:-1].requires_grad_(False)\n",
    "    model.blocks[-1].proj = torch.nn.Linear(\n",
    "        in_features=model.blocks[-1].proj.in_features,\n",
    "        out_features=15\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def make_kinetics_x3d_m():\n",
    "    model = torch.hub.load('facebookresearch/pytorchvideo', 'x3d_m', pretrained=True)\n",
    "    model.blocks[:-1].requires_grad_(False)\n",
    "    model.blocks[-1].proj = torch.nn.Linear(\n",
    "        in_features=model.blocks[-1].proj.in_features,\n",
    "        out_features=15\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def make_kinetics_mvit():\n",
    "    model = torch.hub.load('facebookresearch/pytorchvideo', 'mvit_base_16x4', pretrained=True)\n",
    "    model.blocks[:-1].requires_grad_(False)\n",
    "    model.head.proj = torch.nn.Linear(\n",
    "        in_features=model.head.proj.in_features,\n",
    "        out_features=15\n",
    "    )\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def train(model_fn):\n",
    "    data_module = KineticsDataModule()\n",
    "    classification_module = VideoClassificationLightningModule(\n",
    "        classes=data_module.get_classes(),\n",
    "        model_fn=model_fn,\n",
    "    )\n",
    "    trainer = pytorch_lightning.Trainer(gpus=-1, max_epochs=3)\n",
    "    trainer.fit(classification_module, data_module)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(model_fn=make_kinetics_mvit)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
